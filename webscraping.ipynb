{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0e12bd",
   "metadata": {},
   "source": [
    "\n",
    "# Web Scraping con BeautifulSoup\n",
    "\n",
    "Este cuaderno demuestra cómo realizar web scraping utilizando `requests` y `BeautifulSoup` para extraer el contenido principal de un artículo de una página web, formatearlo en HTML similar a la página original y agregar una referencia a la fuente.\n",
    "\n",
    "## Pasos:\n",
    "1. **Enviar una solicitud HTTP**: Utilizamos `requests` para enviar una solicitud HTTP a la URL de la página web.\n",
    "2. **Analizar el HTML**: Usamos `BeautifulSoup` para analizar el contenido HTML de la página web.\n",
    "3. **Identificar y extraer el contenido**: Buscamos los elementos HTML que contienen el contenido principal del artículo (párrafos y encabezados).\n",
    "4. **Formatear el contenido en HTML**: Estructuramos el contenido extraído en una plantilla HTML.\n",
    "5. **Guardar el contenido en un archivo**: Guardamos el contenido formateado en un archivo HTML para su visualización.\n",
    "\n",
    "A continuación, se muestra el código que implementa estos pasos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def beautifulsoup_web_scrape_url(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Verifica si la solicitud fue exitosa\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al obtener la página web: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_main_content(soup):\n",
    "    # Definir posibles contenedores de contenido\n",
    "    possible_containers = ['article', 'div', 'main', 'section']\n",
    "\n",
    "    for tag in possible_containers:\n",
    "        main_content = soup.find(tag)\n",
    "        if main_content:\n",
    "            paragraphs = main_content.find_all(['p', 'h2', 'h3'])\n",
    "            if paragraphs:\n",
    "                content_html = \"\"\n",
    "                for section in paragraphs:\n",
    "                    content_html += f\"<{section.name}>{section.get_text()}</{section.name}>\n",
    "\"\n",
    "                return content_html\n",
    "\n",
    "    # Si no se encuentra un contenedor claro, buscar todos los párrafos\n",
    "    paragraphs = soup.find_all('p')\n",
    "    if paragraphs:\n",
    "        content_html = \"\"\n",
    "        for p in paragraphs:\n",
    "            content_html += f\"<p>{p.get_text()}</p>\n",
    "\"\n",
    "        return content_html\n",
    "\n",
    "    return \"No se pudo encontrar el contenido del artículo.\"\n",
    "\n",
    "url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6627879/\"\n",
    "soup = beautifulsoup_web_scrape_url(url)\n",
    "\n",
    "if soup:\n",
    "    article_html = extract_main_content(soup)\n",
    "    if article_html:\n",
    "        formatted_html = f'''\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>Artículo</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            {article_html}\n",
    "            <footer>\n",
    "                <p>Fuente: <a href=\"{url}\">{url}</a></p>\n",
    "            </footer>\n",
    "        </body>\n",
    "        </html>\n",
    "        '''\n",
    "        with open(\"article.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(formatted_html)\n",
    "        print(\"El contenido del artículo ha sido formateado y guardado en 'article.html'.\")\n",
    "    else:\n",
    "        print(\"No se pudo extraer el contenido del artículo.\")\n",
    "else:\n",
    "    print(\"Error al obtener la página web.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
