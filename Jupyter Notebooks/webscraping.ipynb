{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping con BeautifulSoup\n",
    "\n",
    "Este cuaderno demuestra cómo realizar web scraping utilizando `requests` y `BeautifulSoup` para extraer el contenido principal de un artículo de una página web, formatearlo en HTML similar a la página original y agregar una referencia a la fuente.\n",
    "\n",
    "## Pasos:\n",
    "1. **Enviar una solicitud HTTP**: Utilizamos `requests` para enviar una solicitud HTTP a la URL de la página web.\n",
    "2. **Analizar el HTML**: Usamos `BeautifulSoup` para analizar el contenido HTML de la página web.\n",
    "3. **Identificar y extraer el contenido**: Buscamos los elementos HTML que contienen el contenido principal del artículo (párrafos y encabezados).\n",
    "4. **Formatear el contenido en HTML**: Estructuramos el contenido extraído en una plantilla HTML.\n",
    "5. **Guardar el contenido en un archivo**: Guardamos el contenido formateado en un archivo HTML para su visualización.\n",
    "\n",
    "A continuación, se muestra el código que implementa estos pasos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables clave:\n",
    "\n",
    "**Variable de entrada**: url (contiene la URL de la página web a analizar).\n",
    "\n",
    "**Variable de salida**: formatted_html (contiene el HTML formateado del contenido principal del artículo).\n",
    "\n",
    "Podemos usar estas variables para enviar el contenido al sistema RAG. Por ejemplo, vamos a pasar formatted_html directamente al bot del RAG para que procese y responda basándose en el contenido extraído.\n",
    "\n",
    "# Integración con RAG:\n",
    "\n",
    "Para enviar formatted_html al sistema RAG, vamos a usar una función específica que se encargue de comunicase con el bot del RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6627879/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El contenido del artículo ha sido formateado y guardado en 'article.html'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import webbrowser\n",
    "\n",
    "# Función para obtener el contenido HTML de una URL usando BeautifulSoup\n",
    "def beautifulsoup_web_scrape_url(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Verifica si la solicitud fue exitosa\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al obtener la página web: {e}\")\n",
    "        return None\n",
    "\n",
    "# Función para extraer el contenido principal del HTML\n",
    "def extract_main_content(soup):\n",
    "    # Definir posibles contenedores de contenido\n",
    "    possible_containers = ['article', 'div', 'main', 'section']\n",
    "\n",
    "    for tag in possible_containers:\n",
    "        main_content = soup.find(tag)\n",
    "        if main_content:\n",
    "            paragraphs = main_content.find_all(['p', 'h2', 'h3'])\n",
    "            if paragraphs:\n",
    "                content_html = \"\"\n",
    "                for section in paragraphs:\n",
    "                    content_html += f\"<{section.name}>{section.get_text()}</{section.name}>\\n\"\n",
    "                return content_html\n",
    "\n",
    "    # Si no se encuentra un contenedor claro, buscar todos los párrafos\n",
    "    paragraphs = soup.find_all('p')\n",
    "    if paragraphs:\n",
    "        content_html = \"\"\n",
    "        for p in paragraphs:\n",
    "            content_html += f\"<p>{p.get_text()}</p>\\n\"\n",
    "        return content_html\n",
    "\n",
    "    return \"No se pudo encontrar el contenido del artículo.\"\n",
    "\n",
    "# Variable de entrada: URL de la página web a estudiar\n",
    "url = \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6627879/\"\n",
    "soup = beautifulsoup_web_scrape_url(url)\n",
    "\n",
    "# Variable de salida: HTML formateado\n",
    "if soup:\n",
    "    article_html = extract_main_content(soup)\n",
    "    if article_html:\n",
    "        formatted_html = f'''\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "            <title>Artículo</title>\n",
    "        </head>\n",
    "        <body>\n",
    "            {article_html}\n",
    "            <footer>\n",
    "                <p>Fuente: <a href=\"{url}\">{url}</a></p>\n",
    "            </footer>\n",
    "        </body>\n",
    "        </html>\n",
    "        '''\n",
    "        file_path = \"article.html\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(formatted_html)\n",
    "        print(\"El contenido del artículo ha sido formateado y guardado en 'article.html'.\")\n",
    "        \n",
    "        # Abrir el archivo HTML en el navegador\n",
    "        webbrowser.open(f\"file://{file_path}\")\n",
    "    else:\n",
    "        print(\"No se pudo extraer el contenido del artículo.\")\n",
    "else:\n",
    "    print(\"Error al obtener la página web.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
