{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidfdezmartin/Chatbot-con-FastAPI-Streamlit-y-LangChain/blob/main/documentos_y_matcomplement/Colab%20de%20Pruebas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avPoFH9T_X8B"
      },
      "source": [
        "Aqu√≠ monto requeriments2.txt del GitHub pero com la libreria typer==0.09.0, que por un lado debe ser <0.10.00 (por eso he puesto 0.09.0 y por otro mayor que para fastapi-cli 0.0.4 requires typer>=0.12.3, a REVISARLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeKeYrcC8o2z"
      },
      "source": [
        "Si se ha instalado longchain y falla, quitar el # del comentario de esta celda.\n",
        "Uso rutas absolutas respecto al GitHub al principio para que no falle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkaYUOpwZ-d6",
        "outputId": "6833a08e-76be-4c72-d05c-11eddc6a35c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: langchain-community==0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: python-dotenv in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: streamlit in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: bs4 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 5)) (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from -r ../requirements2.txt (line 6)) (4.12.3)\n",
            "Requirement already satisfied: pypdf in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 7)) (4.2.0)\n",
            "Requirement already satisfied: faiss-cpu in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 8)) (1.8.0)\n",
            "Requirement already satisfied: groq in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 9)) (0.8.0)\n",
            "Requirement already satisfied: fastapi in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 10)) (0.110.0)\n",
            "Requirement already satisfied: uvicorn in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 11)) (0.28.0)\n",
            "Requirement already satisfied: sse_starlette in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 12)) (2.1.0)\n",
            "Requirement already satisfied: googletrans in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 13)) (2.4.0)\n",
            "Requirement already satisfied: typer==0.09.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 14)) (0.9.0)\n",
            "Requirement already satisfied: wikipedia-api in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from -r ../requirements2.txt (line 15)) (0.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.6.6)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.26.3)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (8.3.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from typer==0.09.0->-r ../requirements2.txt (line 14)) (8.1.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from typer==0.09.0->-r ../requirements2.txt (line 14)) (4.10.0)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (5.3.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (1.8.2)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (5.3.3)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (10.2.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (4.25.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (16.1.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (13.7.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (0.10.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (6.4)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from streamlit->-r ../requirements2.txt (line 4)) (4.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->-r ../requirements2.txt (line 6)) (2.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from groq->-r ../requirements2.txt (line 9)) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from groq->-r ../requirements2.txt (line 9)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from groq->-r ../requirements2.txt (line 9)) (0.26.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from groq->-r ../requirements2.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from fastapi->-r ../requirements2.txt (line 10)) (0.36.3)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn->-r ../requirements2.txt (line 11)) (0.14.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (4.21.1)\n",
            "Requirement already satisfied: toolz in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->groq->-r ../requirements2.txt (line 9)) (3.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->groq->-r ../requirements2.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: colorama in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer==0.09.0->-r ../requirements2.txt (line 14)) (0.4.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r ../requirements2.txt (line 4)) (4.0.11)\n",
            "Requirement already satisfied: certifi in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->groq->-r ../requirements2.txt (line 9)) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->groq->-r ../requirements2.txt (line 9)) (1.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.10.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->-r ../requirements2.txt (line 1)) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->-r ../requirements2.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from rich<14,>=10.14.0->streamlit->-r ../requirements2.txt (line 4)) (2.17.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r ../requirements2.txt (line 4)) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (2.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r ../requirements2.txt (line 4)) (0.17.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r ../requirements2.txt (line 4)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit->-r ../requirements2.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0->-r ../requirements2.txt (line 2)) (1.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#  %pip uninstall -y langchain langchain-community\n",
        "%pip install -r \"../requirements2.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3ulmY2na_zL"
      },
      "source": [
        "SI HACE FALTA WIKI-LANGCHAIN. En celdas posteriores se hacen pruebas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDeVvW7iXd0z",
        "outputId": "ac96d7c0-1b24-497b-dc64-cf2d9d4a0516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet wikipedia langchain langchain_community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNPGlxA3ar9Q"
      },
      "source": [
        "AGENTE DE USUARIO PARA WIKIPEDIA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) Hago una prueba pidiendole a Wiki DIABETES PRIMERO y funciona sin problema. \n",
        "\n",
        "2) Luego hago una prueba de idioma para ver si tb funciona en espa√±ol con una b√∫squeda multiple de t√©rminos\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"C√°ncer\", \"Hipertensi√≥n\"]\n",
        "\n",
        "Cambios probados:\n",
        "Idioma del API de Wikipedia Y PRUEBO UNA BUSQUEDA MULTIPLE:\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "Mensaje de error en espa√±ol:\n",
        "\n",
        "return \"No se encontr√≥ la p√°gina para la consulta: \" + query\n",
        "Estos cambios aseguran que las consultas a Wikipedia se realicen en la versi√≥n en espa√±ol y que cualquier mensaje de error tambi√©n est√© en espa√±ol.\n",
        "\n",
        "RESULTADO: Busca en la wiki en ingles, por lo que los terminos que no son iguales en los 2 idiomas, no los encuentra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrTAmQg-avk8",
        "outputId": "99de8728-f0a0-4d63-8e5e-a7517f7e1a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultado para 'Diabetes':\n",
            "Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects. Classic symptoms include thirst, polyuria, weight loss, and blurred vision. If left untreated, the disease can lead to various health complications, including disorders of the cardiovascular system, eye, kidney, and nerves. Untreated or poorly treated diabetes accounts for approximately 1.5 million deaths every year.\n",
            "The major types of diabetes are type 1 and type 2. The most common treatment for type 1 is insulin replacement therapy (insulin injections), while anti-diabetic medications (such as metformin and semaglutide) and lifestyle modifications can be used to manage type 2. Gestational diabetes, a form that arises during pregnancy in some women, normally resolves shortly after delivery.\n",
            "As of 2021, an estimated 537 million people had diabetes worldwide accounting for 10.5% of the adult population, with type 2 making up about 90% of all cases. It is estimated that by 2045, approximately 783 million adults, or 1 in 8, will be living with diabetes, representing a 46% increase from the current figures. The prevalence of the disease continues to increase, most dramatically in low- and middle-income nations. Rates are similar in women and men, with diabetes being the seventh leading cause of death globally. The global expenditure on diabetes-related healthcare is an estimated US$760 billion a year.\n",
            "\n",
            "Resultado para 'Hemocromatosis':\n",
            "No se encontr√≥ la p√°gina para la consulta: Hemocromatosis\n",
            "\n",
            "Resultado para 'C√°ncer':\n",
            "No se encontr√≥ la p√°gina para la consulta: C√°ncer\n",
            "\n",
            "Resultado para 'Hipertensi√≥n':\n",
            "No se encontr√≥ la p√°gina para la consulta: Hipertensi√≥n\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import wikipediaapi\n",
        "\n",
        "# Configuraci√≥n de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No se encontr√≥ la p√°gina para la consulta: \" + query\n",
        "\n",
        "# Ejemplo de uso con varios t√©rminos en espa√±ol\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"C√°ncer\", \"Hipertensi√≥n\"]\n",
        "for query in queries:\n",
        "    result = get_wikipedia_summary(query)\n",
        "    print(f\"Resultado para '{query}':\\n{result}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPTCzk1ebdf6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l9vWb27cmKH"
      },
      "source": [
        "AHORA INTEGRAMOS LANGCHAIN CON WIKI Y PROBAMOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXS3LQ21deFF",
        "outputId": "e7bee3c7-9ccb-4ed4-eca4-2eb692c88e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (2.7.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: tqdm in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (1.26.3)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (1.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sentence_transformers) (0.23.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (10.2.0)\n",
            "Requirement already satisfied: filelock in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2021.4.0)\n",
            "Requirement already satisfied: colorama in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence_transformers) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence_transformers) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\casti\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in d:\\users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA-C3lf_esDR"
      },
      "source": [
        "PRUEBA DE API KEY DE HUGGING FACE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztVtxmQIAirG"
      },
      "source": [
        "Me funciona muy bien subiendo el .env de GitHub al root de aqu√≠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdO1rIukertP",
        "outputId": "02542965-9cba-4b42-e207-53ad0cd861ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to C:\\Users\\casti\\.cache\\huggingface\\token\n",
            "Login successful\n",
            "GROQ_API_KEY cargado correctamente\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "add_to_git_credential=True\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    from huggingface_hub import login\n",
        "    login(hf_token)\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Obtener otras variables de entorno si es necesario\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "    print(\"GROQ_API_KEY cargado correctamente\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY no encontrado en el archivo .env\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todo sale bien, pero si se quiere quitar el aviso en rojo debe ponerse en un .sh o correr en la terminal: git config --global credential.helper store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: read).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to C:\\Users\\casti\\.cache\\huggingface\\token\n",
            "Login successful\n",
            "HUGGING_FACE_API_TOKEN cargado correctamente\n",
            "GROQ_API_KEY cargado correctamente\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    from huggingface_hub import login\n",
        "    login(token=hf_token, add_to_git_credential=True)  # Agregar el par√°metro aqu√≠\n",
        "    print(\"HUGGING_FACE_API_TOKEN cargado correctamente\")\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Obtener otras variables de entorno si es necesario\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "    print(\"GROQ_API_KEY cargado correctamente\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY no encontrado en el archivo .env\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pg-fC0e56F"
      },
      "source": [
        "AHORA PRUEBA DE CODIGO BUENO\n",
        "Explicaci√≥n\n",
        "VectorStoreRetriever: Se utiliza para configurar el recuperador basado en el vectorstore.\n",
        "load_qa_chain: Carga una cadena de preguntas y respuestas utilizando el PromptTemplate.\n",
        "RetrievalQA: Se configura con el retriev:\n",
        "D:\\GitHub\\TFM\\Chatbot-con-FastAPI-Streamlit-y-LangChain-1\\documentos_y_matcomplement\\docuentreno\\md\\Cholesterol-Myths-vs-Facts-Spanish.md\n",
        "\n",
        "Pruebo con un pdf pasado a .md de David En PDF\n",
        "\n",
        "RESULTADO: LO CARGA bien, en MD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JevHfNjqf3TG"
      },
      "outputs": [],
      "source": [
        "CARGAR=\"D:\\GitHub\\TFM\\Chatbot-con-FastAPI-Streamlit-y-LangChain-1\\documentos_y_matcomplement\\docuentreno\\md\\Cholesterol-Myths-vs-Facts-Spanish.md\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PRUEBA GENERAL CON WIKI Y LANGCHAIN: USAMOS MODELO DE HUGGING FACE GPT2.\n",
        "Creamos una variable para poder cambiar el numero maximo de tokens de entrada y salida si cambiamos el modelo facilmente.\n",
        "\n",
        "Explicaci√≥n.\n",
        "a) Definir el l√≠mite de tokens: Se establece MAX_INPUT_TOKENS para los tokens de entrada y MAX_NEW_TOKENS para los nuevos tokens generados.\n",
        "\n",
        "b) Creamos una funci√≥n para recortar el contexto: truncate_context recorta el contexto para asegurarse de que no se exceda el l√≠mite de tokens.\n",
        "\n",
        "c) Crear el prompt con el contexto recortado: Se asegura de que el prompt total no exceda el l√≠mite de tokens permitido por el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explicaci√≥n de las funciones de control de tokens\n",
        "\n",
        "Hemos implementado funciones de control de tokens en nuestro c√≥digo para asegurar que el n√∫mero total de tokens (entrada m√°s salida) no exceda el l√≠mite permitido por el modelo `gpt2`. Aqu√≠ est√° la explicaci√≥n de las funciones clave que hemos creado y c√≥mo puedes ajustarlas cuando cambies de modelo.\n",
        "\n",
        "#### Funci√≥n `truncate_context`\n",
        "\n",
        "Esta funci√≥n se utiliza para recortar el contexto de entrada de modo que el n√∫mero total de tokens no exceda el l√≠mite permitido. Utilizamos el `GPT2Tokenizer` para contar y recortar los tokens.\n",
        "\n",
        "**Par√°metros:**\n",
        "- `context`: El texto de contexto que se desea truncar.\n",
        "- `max_tokens`: El n√∫mero m√°ximo de tokens permitidos para el contexto.\n",
        "\n",
        "**Descripci√≥n:**\n",
        "- La funci√≥n tokeniza el contexto usando `tokenizer.encode`.\n",
        "- Si el n√∫mero de tokens excede `max_tokens`, se truncan los tokens al l√≠mite especificado.\n",
        "- La funci√≥n devuelve el contexto truncado como texto utilizando `tokenizer.decode`.\n",
        "\n",
        "#### Definici√≥n de l√≠mites de tokens\n",
        "\n",
        "Hemos definido los l√≠mites de tokens de entrada (`MAX_INPUT_TOKENS`) y salida (`MAX_NEW_TOKENS`) para asegurarnos de que el n√∫mero total de tokens no exceda 1024, el l√≠mite para `gpt2`.\n",
        "**Descripci√≥n:**\n",
        "- `MAX_INPUT_TOKENS`: El n√∫mero m√°ximo de tokens permitidos para el contexto de entrada.\n",
        "- `MAX_NEW_TOKENS`: El n√∫mero m√°ximo de tokens permitidos para la respuesta generada por el modelo.\n",
        "\n",
        "#### Aplicaci√≥n en el c√≥digo\n",
        "\n",
        "Utilizamos la funci√≥n `truncate_context` para recortar el contexto antes de crear el prompt y enviar la solicitud al modelo.\n",
        "\n",
        "### Ajustes para cambiar de modelo\n",
        "\n",
        "Cuando se cambie de modelo, necesitaremos ajustar los l√≠mites de tokens (`MAX_INPUT_TOKENS` y `MAX_NEW_TOKENS`) seg√∫n las especificaciones del nuevo modelo. La mayor√≠a de los modelos tienen documentaci√≥n que indica el l√≠mite m√°ximo de tokens que pueden manejar. Por ejemplo, si cambia a un modelo con un l√≠mite de 2048 tokens, podr√≠as ajustar los l√≠mites de la siguiente manera:\n",
        "\n",
        "```python\n",
        "MAX_INPUT_TOKENS = 1948  # Ajustar seg√∫n el nuevo l√≠mite\n",
        "MAX_NEW_TOKENS = 100\n",
        "```\n",
        "\n",
        "Asegurarse de revisar la documentaci√≥n del nuevo modelo para conocer el l√≠mite exacto de tokens y ajustar las variables en consecuencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultado para 'Diabetes':\n",
            "Diabetes mellitus, often known simply as diabetes, is a group of common endocrine diseases characterized by sustained high blood sugar levels. Diabetes is due to either the pancreas not producing enough insulin, or the cells of the body becoming unresponsive to the hormone's effects. Classic symptoms include thirst, polyuria, weight loss, and blurred vision. If left untreated, the disease can lead to various health complications, including disorders of the cardiovascular system, eye, kidney, and nerves. Untreated or poorly treated diabetes accounts for approximately 1.5 million deaths every year.\n",
            "The major types of diabetes are type 1 and type 2. The most common treatment for type 1 is insulin replacement therapy (insulin injections), while anti-diabetic medications (such as metformin and semaglutide) and lifestyle modifications can be used to manage type 2. Gestational diabetes, a form that arises during pregnancy in some women, normally resolves shortly after delivery.\n",
            "As of 2021, an estimated 537 million people had diabetes worldwide accounting for 10.5% of the adult population, with type 2 making up about 90% of all cases. It is estimated that by 2045, approximately 783 million adults, or 1 in 8, will be living with diabetes, representing a 46% increase from the current figures. The prevalence of the disease continues to increase, most dramatically in low- and middle-income nations. Rates are similar in women and men, with diabetes being the seventh leading cause of death globally. The global expenditure on diabetes-related healthcare is an estimated US$760 billion a year.\n",
            "\n",
            "Resultado para 'Hemocromatosis':\n",
            "No se encontr√≥ la p√°gina para la consulta: Hemocromatosis\n",
            "\n",
            "Resultado para 'C√°ncer':\n",
            "No se encontr√≥ la p√°gina para la consulta: C√°ncer\n",
            "\n",
            "Resultado para 'Hipertensi√≥n':\n",
            "No se encontr√≥ la p√°gina para la consulta: Hipertensi√≥n\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n",
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `RetrievalQA` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use create_retrieval_chain instead.\n",
            "  warn_deprecated(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2310 > 1024). Running this sequence through the model will result in indexing errors\n",
            "d:\\Users\\casti\\pinokio\\bin\\miniconda\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA Result:\n",
            " Responde la pregunta bas√°ndote en el siguiente contexto: Ôªø<a name=\"br1\"></a> \n",
            "\n",
            "COLESTEROL: MITOS Y VERDADES\n",
            "\n",
            "Mito: S√≥lo necesitar√° analizar sus niveles de colesterol cuando tenga m√°s de 30 a√±os.\n",
            "\n",
            "Verdad: La Asociaci√≥n Americana del Coraz√≥n (AHA) recomienda analizar los niveles de colesterol\n",
            "\n",
            "una vez entre los 9 y 11 a√±os, y de nuevo entre los 17 y 21 a√±os para ni√±os y adultos j√≥venes sin otros\n",
            "\n",
            "factores de riesgos o un historial familiar de enfermedad card√≠aca temprana. Despu√©s de los 20 factores de riesgos o un historial familiar de enfermedad card√≠aca temprana. Despu√©s de los 20\n",
            "\n",
            "a√±os, su doctor volver√° a analizar su colesterol y otros factores de riesgo cada cuatro a seis a√±os\n",
            "\n",
            "siempre y cuando su riesgo se mantenga bajo.\n",
            "\n",
            "Mito: S√≥lo la gente con sobrepeso u obesidad tiene colesterol alto.\n",
            "\n",
            "Verdad: Las personas con cualquier tipo de cuerpo pueden tener colesterol alto. Tener sobrepeso Verdad: Las personas con cualquier tipo de cuerpo pueden tener colesterol alto. Tener sobrepeso\n",
            "\n",
            "u obesidad aumenta sus probabilidades de tener colesterol alto, pero ser delgado no lo protege.\n",
            "\n",
            "Independientemente de su peso, dieta y nivel de actividad f√≠sica, deber√≠a medir su nivel de\n",
            "\n",
            "colesterol con regularidad.\n",
            "\n",
            "Mito: Tener colesterol alto es un problema s√≥lo de los hombres.\n",
            "\n",
            "Verdad: Aunque la ateroesclerosis generalmente ocurre a una mayor edad en las mujeres que Verdad: Aunque la ateroesclerosis generalmente ocurre a una mayor edad en las mujeres que\n",
            "\n",
            "en los hombres, la enfermedad cardiovascular sigue siendo la principal causa de muerte en las\n",
            "\n",
            "mujeres. Los m√©dicos deben considerar condiciones medicas especiÔ¨Åcas de mujeres, tales como\n",
            "\n",
            "menopausia prematura (antes de los 40 a√±os) y condiciones asociadas al embarazo, cuando les\n",
            "\n",
            "hablen sobre sus niveles de colesterol y las opciones para su tratamiento. hablen sobre sus niveles de colesterol y las opciones para su tratamiento.\n",
            "\n",
            "Mito: Si su m√©dico no ha mencionado sus niveles de colesterol, usted est√° bien.\n",
            "\n",
            "Verdad: Usted puede hacerse cargo de su salud. Si usted tiene 20 a√±os o m√°s, p√≠dale a su m√©dico\n",
            "\n",
            "que le realice un an√°lisis de colesterol, que eval√∫e sus factores de riesgo y que determine su riesgo\n",
            "\n",
            "de tener un ataque al coraz√≥n o un derrame cerebral. Si tiene entre 20 y 39 a√±os, su m√©dico deber√≠a evaluar su ries\n",
            "Pregunta: ¬øCu√°l es el tema principal del documento? ¬øP√∫blico, en muy de la meretico\n",
            "\n",
            "seo puede los niveles de colesterol por cada del pol√≠ticas, que la quesar el\n",
            "\n",
            "municiones a la pa√≠s, una salud el tema principal del documento? ¬øCu√°l es el tema principal del documento?\n",
            "\n",
            "Ser√≠a: P√∫blico se muy en las hombres\n"
          ]
        }
      ],
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Usar root como base del GitHub\n",
        "root = \"https://github.com/davidfdezmartin/Chatbot-con-FastAPI-Streamlit-y-LangChain/blob/main\"\n",
        "\n",
        "# Verificar y obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Configuraci√≥n de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No se encontr√≥ la p√°gina para la consulta: \" + query\n",
        "\n",
        "# Ejemplo de uso con varios t√©rminos en espa√±ol\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"C√°ncer\", \"Hipertensi√≥n\"]\n",
        "for query in queries:\n",
        "    result = get_wikipedia_summary(query)\n",
        "    print(f\"Resultado para '{query}':\\n{result}\\n\")\n",
        "\n",
        "# URL del archivo en GitHub\n",
        "file_url = f\"{root}/documentos_y_matcomplement/docuentreno/md/Cholesterol-Myths-vs-Facts-Spanish.md?raw=true\"\n",
        "\n",
        "# Descargar el contenido del archivo\n",
        "response = requests.get(file_url)\n",
        "if response.status_code != 200:\n",
        "    raise RuntimeError(f\"Error al descargar el archivo desde {file_url}\")\n",
        "\n",
        "# Guardar el contenido en un archivo temporal\n",
        "with open(\"temp.md\", \"w\", encoding=\"utf-8\") as temp_file:\n",
        "    temp_file.write(response.text)\n",
        "\n",
        "# Cargar el documento .md desde el archivo temporal\n",
        "text_loader = TextLoader(\"temp.md\")\n",
        "documents = text_loader.load()\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reducir el tama√±o del fragmento\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta bas√°ndote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Definir el l√≠mite de tokens\n",
        "MAX_INPUT_TOKENS = 824  # Ajustar seg√∫n sea necesario\n",
        "MAX_NEW_TOKENS = 100\n",
        "TOTAL_TOKENS = MAX_INPUT_TOKENS + MAX_NEW_TOKENS\n",
        "\n",
        "# Inicializar el tokenizer de GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Funci√≥n para recortar el contexto si es necesario\n",
        "def truncate_context(context, max_tokens):\n",
        "    tokens = tokenizer.encode(context)\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "        return tokenizer.decode(tokens, clean_up_tokenization_spaces=True)\n",
        "    return context\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=hf_token)\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas con control de longitud de tokens\n",
        "question = \"¬øCu√°l es el tema principal del documento?\"\n",
        "\n",
        "# Obtener el contexto\n",
        "context = \" \".join([doc.page_content for doc in texts])\n",
        "context = truncate_context(context, MAX_INPUT_TOKENS)\n",
        "\n",
        "# Crear el prompt con el contexto recortado\n",
        "prompt = prompt_template.format(context=context, question=question)\n",
        "\n",
        "# Generar la respuesta utilizando el modelo de lenguaje\n",
        "result = llm(prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "print(\"QA Result:\\n\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------REVISADO HASTA AQU√ç EL 28/05 14------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-o4e4A-1Y28"
      },
      "source": [
        "PRUEBA 2. ASI CARGARIAMOS LOS PDF DE SU SUBDIRECTORIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "PXxZNfk41QeH",
        "outputId": "6b1fd0db-5df7-4edc-85e0-e7cf8cb3367f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'PyPDFDirectoryLoader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a3bbfe67377>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Read the ppdfs from the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPyPDFDirectoryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PyPDFDirectoryLoader' is not defined"
          ]
        }
      ],
      "source": [
        "## Read the ppdfs from the folder\n",
        "loader=PyPDFDirectoryLoader(\"./pdf\")\n",
        "\n",
        "documents=loader.load()\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "\n",
        "final_documents=text_splitter.split_documents(documents)\n",
        "final_documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkt3WOT4EGyD"
      },
      "source": [
        "La anterior PRUEBA 1 (de PDF modificado)., lo rehago para cargar .md, y adem√°s sigo haciendo pruebas con WIKIPEDIA que fuciona perfecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUj79yYxHeeg"
      },
      "source": [
        "El error normal de longitud indica que el n√∫mero de tokens en la entrada m√°s los tokens generados exceden el l√≠mite de 1024 para el modelo gpt2. Para resolver esto, podemos:\n",
        "\n",
        "Reducir el tama√±o del contexto enviado al modelo.\n",
        "Utilizar un modelo con un l√≠mite de tokens mayor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljP-ISvlEF5-",
        "outputId": "dbd1090b-23a2-4c0e-ee93-f58088f0975c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token de Hugging Face cargado correctamente.\n",
            "Wikipedia Result:\n",
            " Hunter √ó Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha's sh≈çnen manga magazine Weekly Sh≈çnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 37 tank≈çbon volumes as of November 2022. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\n",
            "Hunter √ó Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter √ó Hunter.\n",
            "The manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim's Toonami programming block from April 2016 to June 2019.\n",
            "Hunter √ó Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA Result:\n",
            " {'query': '¬øCu√°l es el tema principal del documento?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nde calor√≠as. Adem√°s, tenga en cuenta que la porci√≥n en que se basan esos n√∫meros puede ser menor\\n\\nque el contenido completo del envase. (Es decir, puede que el envase incluya m√°s de una porci√≥n).\\n\\nMito: Cambiar la mantequilla por margarina me ayudar√° a bajar el colesterol.\\n\\nVerdad: No necesariamente. La mantequilla es alta en grasa saturada, pero algunos tipos de\\n\\nmargarinas tienen incluso un contenido mayor de ambos tipos de grasas. Las margarinas l√≠quidas\\n\\nfactores de riesgos o un historial familiar de enfermedad card√≠aca temprana. Despu√©s de los 20\\n\\na√±os, su doctor volver√° a analizar su colesterol y otros factores de riesgo cada cuatro a seis a√±os\\n\\nsiempre y cuando su riesgo se mantenga bajo.\\n\\nMito: S√≥lo la gente con sobrepeso u obesidad tiene colesterol alto.\\n\\nVerdad: Las personas con cualquier tipo de cuerpo pueden tener colesterol alto. Tener sobrepeso\\n\\nminutos a la semana, o una combinaci√≥n de ambas, de preferencia esparcidas durante la semana.\\n\\nMito: Si la etiqueta de nutrici√≥n no muestra colesterol, el alimento es saludable para el coraz√≥n.\\n\\nVerdad: Muchos alimentos que ‚Äúno tienen colesterol‚Äù o incluso que son ‚Äúbajos en grasa‚Äù tienen un\\n\\nalto contenido de otros tipos de grasas ‚Äúmalas‚Äù, como las grasas saturadas y grasas trans. Aseg√∫rese\\n\\nde revisar la etiqueta de los alimentos para ver si tienen grasa saturada y grasa trans, y ver el total\\n\\nevaluar su riesgo para toda la vida. Si tiene entre 40 y 75 a√±os, p√≠dale a su m√©dico que eval√∫e su\\n\\nriesgo a 10 a√±os. Si sus riesgos son altos, un cambio en su estilo de vida y medicamento de estatina\\n\\npodr√≠an ayudar a controlar el riesgo.\\n\\nMito: Su nivel de colesterol es consecuencia de su dieta y su nivel de actividad f√≠sica.\\n\\nVerdad: Es verdad, la dieta y la actividad f√≠sica afectan su nivel de colesterol, pero no son los √∫nicos\\n\\nQuestion: ¬øCu√°l es el tema principal del documento?\\nHelpful Answer: There are 13 documents that are part of the document. Each of these documents is a document that may be part of the document.\\n\\nDe cambio de la vida, el alimento de dos juego a los estos que √©quipar√≠a, durante que la vida se proceso a conocido de una logar√≠a\\n\\ncon el enfuerzo un vida.\\n\\nVerdad: ¬øCu√°l es\"}\n"
          ]
        }
      ],
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader  # Aseg√∫rate de que el import es correcto\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno\n",
        "load_dotenv()\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    print(\"Token de Hugging Face cargado correctamente.\")\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Configuraci√≥n de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No page found for query: \" + query\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"Hunter x Hunter\"\n",
        "wikipedia_result = get_wikipedia_summary(query)\n",
        "\n",
        "print(\"Wikipedia Result:\\n\", wikipedia_result)\n",
        "\n",
        "# Configuraci√≥n de LangChain y otros m√≥dulos\n",
        "# Cargar documentos\n",
        "CARGAR = \"/content/sample_data/Cholesterol-Myths-vs-Facts-Spanish.md\"\n",
        "\n",
        "# Leer el archivo de texto manualmente\n",
        "with open(CARGAR, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Crear una clase de documento simple\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "# Crear un objeto de documento\n",
        "documents = [Document(page_content=text)]\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reducir el tama√±o del fragmento\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta bas√°ndote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Configurar el modelo de lenguaje de Hugging Face\n",
        "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=hf_token)\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas\n",
        "question = \"¬øCu√°l es el tema principal del documento?\"\n",
        "result = retrieval_qa(question)\n",
        "\n",
        "print(\"QA Result:\\n\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5m2xY9MHzB_"
      },
      "source": [
        "VOY REVISANDO POR AQU√ç HASTA AHORA CARGA DE MD Y LLM DE HUGGING FACES PERFECTO Y WIKIPEDIA PERFECTA\n",
        "\n",
        "Hoy 29/04 se ha revisado para que cargue los documentos a partir del root de github para hacer el codigo portable y no dependiente de la copia local del github particular.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CODIGO CLON DEL ANTERIOR USAN .ENV Y ROOT PARA ACORTAR DONDE APUNTAN LOS PROGRAMAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Usar root como base del GitHub\n",
        "root = os.getenv(\"ROOT\")\n",
        "if not root:\n",
        "    raise ValueError(\"ROOT no encontrado en el archivo .env\")\n",
        "\n",
        "print(\"Root de GitHub:\", root)\n",
        "\n",
        "# Verificar y obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if not hf_token:\n",
        "    raise ValueError(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Configuraci√≥n de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('es', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No se encontr√≥ la p√°gina para la consulta: \" + query\n",
        "\n",
        "# Ejemplo de uso con varios t√©rminos en espa√±ol\n",
        "queries = [\"Diabetes\", \"Hemocromatosis\", \"C√°ncer\", \"Hipertensi√≥n\"]\n",
        "for query in queries:\n",
        "    result = get_wikipedia_summary(query)\n",
        "    print(f\"Resultado para '{query}':\\n{result}\\n\")\n",
        "\n",
        "# URL del archivo en GitHub\n",
        "file_path = os.path.join(root, \"blob/main/documentos_y_matcomplement/docuentreno/md/Cholesterol-Myths-vs-Facts-Spanish.md?raw=true\")\n",
        "\n",
        "# Descargar el contenido del archivo\n",
        "response = requests.get(file_path)\n",
        "if response.status_code != 200:\n",
        "    raise RuntimeError(f\"Error al descargar el archivo desde {file_path}\")\n",
        "\n",
        "# Guardar el contenido en un archivo temporal\n",
        "with open(\"temp.md\", \"w\", encoding=\"utf-8\") as temp_file:\n",
        "    temp_file.write(response.text)\n",
        "\n",
        "# Cargar el documento .md desde el archivo temporal\n",
        "text_loader = TextLoader(\"temp.md\")\n",
        "documents = text_loader.load()\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reducir el tama√±o del fragmento\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta bas√°ndote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Definir el l√≠mite de tokens\n",
        "MAX_INPUT_TOKENS = 824  # Ajustar seg√∫n sea necesario\n",
        "MAX_NEW_TOKENS = 100\n",
        "TOTAL_TOKENS = MAX_INPUT_TOKENS + MAX_NEW_TOKENS\n",
        "\n",
        "# Inicializar el tokenizer de GPT-2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Funci√≥n para recortar el contexto si es necesario\n",
        "def truncate_context(context, max_tokens):\n",
        "    tokens = tokenizer.encode(context)\n",
        "    if len(tokens) > max_tokens:\n",
        "        tokens = tokens[:max_tokens]\n",
        "        return tokenizer.decode(tokens, clean_up_tokenization_spaces=True)\n",
        "    return context\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=hf_token)\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas con control de longitud de tokens\n",
        "question = \"¬øCu√°l es el tema principal del documento?\"\n",
        "\n",
        "# Obtener el contexto\n",
        "context = \" \".join([doc.page_content for doc in texts])\n",
        "context = truncate_context(context, MAX_INPUT_TOKENS)\n",
        "\n",
        "# Crear el prompt con el contexto recortado\n",
        "prompt = prompt_template.format(context=context, question=question)\n",
        "\n",
        "# Generar la respuesta utilizando el modelo de lenguaje\n",
        "result = llm(prompt, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "print(\"QA Result:\\n\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "3xQRZ6wW1QeI",
        "outputId": "e4f372df-66cd-4805-aa45-5a061bd86d54"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'final_documents' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-af67d951323e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'final_documents' is not defined"
          ]
        }
      ],
      "source": [
        "len(final_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yypRm5wy1QeI",
        "outputId": "cde70545-8775-4c4f-9912-d4dc33264c6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "## Embedding Using Huggingface\n",
        "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\",      #sentence-transformers/all-MiniLM-l6-v2\n",
        "    model_kwargs={'device':'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings':True}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUpiY9gc1QeI",
        "outputId": "3c1b87e4-48ea-4826-f05f-ec34fe99923d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-8.46568644e-02 -1.19099217e-02 -3.37892585e-02  2.94559337e-02\n",
            "  5.19159958e-02  5.73839322e-02 -4.10017520e-02  2.74268016e-02\n",
            " -1.05128214e-01 -1.58056132e-02  7.94858634e-02  5.64318486e-02\n",
            " -1.31765157e-02 -3.41543928e-02  5.81604475e-03  4.72547710e-02\n",
            " -1.30746774e-02  3.12988879e-03 -3.44225690e-02  3.08406260e-02\n",
            " -4.09086421e-02  3.52737904e-02 -2.43761651e-02 -4.35831733e-02\n",
            "  2.41503324e-02  1.31986588e-02 -4.84452769e-03  1.92347374e-02\n",
            " -5.43912649e-02 -1.42735064e-01  5.15528210e-03  2.93115843e-02\n",
            " -5.60810715e-02 -8.53536371e-03  3.14141326e-02  2.76736189e-02\n",
            " -2.06188373e-02  8.24231505e-02  4.15425114e-02  5.79654835e-02\n",
            " -3.71587239e-02  6.26157876e-03 -2.41390076e-02 -5.61792636e-03\n",
            " -2.51715183e-02  5.04967198e-03 -2.52801236e-02 -2.91945064e-03\n",
            " -8.24046135e-03 -5.69604561e-02  2.30822619e-02 -5.54219959e-03\n",
            "  5.11555411e-02  6.09937944e-02  6.49766475e-02 -5.38514033e-02\n",
            "  2.19109710e-02 -2.54194364e-02 -4.49223518e-02  4.22459245e-02\n",
            "  4.75252084e-02  7.23217207e-04 -2.61084497e-01  9.30173397e-02\n",
            "  1.13597196e-02  4.90668938e-02 -1.06287086e-02 -8.08727555e-03\n",
            " -1.53562622e-02 -5.33785969e-02 -6.89967200e-02  4.75178175e-02\n",
            " -5.68596013e-02  9.38643236e-03  4.24065925e-02  2.54346598e-02\n",
            "  9.67096724e-03  7.90799968e-03  2.25160886e-02  1.91007671e-03\n",
            "  3.06091923e-02  2.43992303e-02 -1.34115480e-02 -4.77401279e-02\n",
            "  4.89939749e-02 -9.49416459e-02  5.62894158e-02 -4.76260781e-02\n",
            "  2.81447303e-02 -2.54329499e-02 -3.84951308e-02  1.00940112e-02\n",
            "  1.90582985e-04  3.36625241e-02  1.00181838e-02  2.83524077e-02\n",
            " -2.68963701e-03 -6.96359901e-03 -3.54914516e-02  3.42758894e-01\n",
            " -1.94496289e-02  1.43988123e-02 -5.68810571e-03  1.71480775e-02\n",
            " -2.88607483e-03 -5.81653193e-02  6.35178352e-04  5.17297909e-03\n",
            "  2.06331387e-02  1.65708251e-02  2.15096809e-02 -2.38796007e-02\n",
            "  2.89275143e-02  4.67319153e-02 -3.56104821e-02 -1.05078742e-02\n",
            "  3.70704755e-02  1.57502498e-02  9.43095461e-02 -2.50715371e-02\n",
            " -9.55965184e-03  1.78565886e-02 -9.41779092e-03 -4.57858741e-02\n",
            "  1.82930417e-02  5.81431836e-02  4.94311154e-02  1.46350682e-01\n",
            "  2.16057692e-02 -3.92896198e-02  1.03241250e-01 -3.48299928e-02\n",
            " -6.61871349e-03  7.07991235e-03  9.26990295e-04  4.49867966e-03\n",
            " -2.89777480e-02  4.02419232e-02 -5.23192994e-03  4.59961966e-02\n",
            "  4.23976406e-03 -4.83790739e-03 -3.23238224e-03 -1.41072914e-01\n",
            " -3.76811475e-02  1.83623895e-01 -2.96609867e-02  4.90660444e-02\n",
            "  3.90551575e-02 -1.57757346e-02 -3.86351012e-02  4.65630889e-02\n",
            " -2.43486036e-02  3.57695147e-02 -3.54947336e-02  2.36265883e-02\n",
            " -3.41984764e-04  3.11703496e-02 -2.39356644e-02 -5.94757982e-02\n",
            "  6.06259257e-02 -3.81902158e-02 -7.04255328e-02  1.42479865e-02\n",
            "  3.34432051e-02 -3.85255218e-02 -1.71951596e-02 -7.12288395e-02\n",
            "  2.64976211e-02  1.09495688e-02  1.32650323e-02  3.89527939e-02\n",
            "  1.60355382e-02 -3.17630395e-02  1.02013692e-01  2.92911958e-02\n",
            " -2.29205769e-02 -8.38053040e-03 -1.72173064e-02 -6.78820610e-02\n",
            "  5.39418403e-03 -2.32346989e-02 -6.07407168e-02 -3.86575758e-02\n",
            " -1.54306367e-02 -3.84982936e-02 -5.02867699e-02  5.04235253e-02\n",
            "  4.94898260e-02 -1.41083300e-02 -2.98144598e-03  9.76440133e-05\n",
            " -6.59190416e-02  3.01006734e-02 -5.46592870e-04 -1.64787639e-02\n",
            " -5.21614477e-02 -3.30222957e-03  4.75748219e-02 -3.40808518e-02\n",
            " -2.98659913e-02  2.75014956e-02  5.90203749e-03 -2.64040008e-03\n",
            " -1.61242671e-02  2.05222610e-02  1.21105220e-02 -5.49782291e-02\n",
            "  5.10389581e-02 -7.92088546e-03  7.25206453e-03  3.51751335e-02\n",
            "  3.66276912e-02  5.67682087e-04  2.60788649e-02  2.50971057e-02\n",
            "  1.14481300e-02 -2.54925024e-02  1.96417440e-02  2.84220409e-02\n",
            "  2.82554235e-02  6.57489598e-02  9.26554129e-02 -2.68629670e-01\n",
            " -8.90553114e-04  3.16917268e-03  5.08358562e-03 -6.42101169e-02\n",
            " -4.56614904e-02 -4.62259948e-02  3.60924639e-02  8.29056371e-03\n",
            "  8.92349407e-02  5.68022132e-02  6.91062305e-03 -1.08684311e-02\n",
            "  9.36060175e-02  1.03680165e-02 -8.60929266e-02  1.77331921e-02\n",
            " -2.00802702e-02 -1.85124874e-02  5.62404632e-04 -9.38337017e-03\n",
            "  7.76061229e-03 -5.37273772e-02 -2.30028406e-02  7.48890713e-02\n",
            " -1.29693234e-02  6.53716922e-02 -4.24983352e-02 -7.10293651e-02\n",
            " -1.56803615e-02 -6.23028576e-02  5.36034741e-02 -6.53212238e-03\n",
            " -1.15985490e-01  6.70967922e-02  1.93367060e-02 -6.67827874e-02\n",
            " -2.01754435e-03 -6.27636909e-02 -2.95005478e-02 -2.71986630e-02\n",
            "  4.49796803e-02 -6.61587194e-02  2.13751234e-02 -2.94077471e-02\n",
            " -5.71503267e-02  4.05282490e-02  7.11039603e-02 -6.80165365e-02\n",
            "  2.11908873e-02  1.30515285e-02 -2.91152820e-02 -2.25581583e-02\n",
            " -1.60188414e-02  3.20554040e-02 -5.89460693e-02 -2.97131632e-02\n",
            "  3.42681594e-02 -1.58376191e-02 -9.31771472e-03  3.59834097e-02\n",
            "  3.65337380e-03  4.73319739e-02 -1.06235184e-02 -8.69734772e-03\n",
            " -4.38009948e-02  5.94554143e-03 -2.41493657e-02 -7.79940188e-02\n",
            "  1.46542490e-02  1.05613861e-02  5.45365028e-02 -3.17896828e-02\n",
            " -1.26762642e-02  7.92561378e-03 -1.38133150e-02  5.01396768e-02\n",
            " -7.28576025e-03 -5.23702893e-03 -5.32640517e-02  4.78208959e-02\n",
            " -5.38353659e-02  1.11437477e-02  3.96674089e-02 -1.93496253e-02\n",
            "  9.94823873e-03 -3.53479455e-03  3.58561263e-03 -9.61500779e-03\n",
            "  2.15323791e-02 -1.82350334e-02 -2.15188656e-02 -1.38835954e-02\n",
            " -1.76698975e-02  3.38015234e-04 -3.84854589e-04 -2.25800529e-01\n",
            "  4.51243371e-02  1.53376386e-02 -1.76966917e-02 -1.42525993e-02\n",
            " -7.00285891e-03 -3.13724913e-02  2.13672244e-03 -9.28345323e-03\n",
            " -1.66986901e-02  4.66264114e-02  7.71809518e-02  1.26696959e-01\n",
            " -1.83595419e-02 -1.39637077e-02 -1.23306655e-03  5.93339056e-02\n",
            " -1.37463736e-03  1.98233537e-02 -2.92635858e-02  4.96656746e-02\n",
            " -6.07207343e-02  1.53544754e-01 -4.67309207e-02  1.97028890e-02\n",
            " -7.67833516e-02 -7.73231685e-03  3.71618718e-02 -3.00591048e-02\n",
            "  8.30263086e-03  2.06259061e-02  1.97466253e-03  3.39764208e-02\n",
            " -1.70869529e-02  4.84796055e-02  1.20781595e-02  1.24999117e-02\n",
            "  5.61724417e-02  9.88546945e-03  2.13879198e-02 -4.25293520e-02\n",
            " -1.94036588e-02  2.47837696e-02  1.37260715e-02  6.41119629e-02\n",
            " -2.84481011e-02 -4.64116633e-02 -5.36255538e-02 -6.95093986e-05\n",
            "  6.45710081e-02 -4.32005967e-04 -1.32471016e-02  5.85135119e-03\n",
            "  1.48595814e-02 -5.41847497e-02 -2.02038661e-02 -5.98262921e-02\n",
            "  3.67029347e-02  1.43319997e-03 -8.64463206e-03  2.90671345e-02\n",
            "  4.38365377e-02 -7.64942840e-02  1.55717917e-02  6.65831119e-02]\n",
            "(384,)\n"
          ]
        }
      ],
      "source": [
        "import  numpy as np\n",
        "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
        "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMWXH7-11QeI"
      },
      "outputs": [],
      "source": [
        "## VectorStore Creation\n",
        "vectorstore=FAISS.from_documents(final_documents[:120],huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SH9QYTK1QeI",
        "outputId": "233503b8-c137-45e5-95b3-83f47dc8a153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 U.S. Census Bureau\n",
            "WHAT IS HEALTH INSURANCE COVERAGE?\n",
            "This brief presents state-level estimates of health insurance coverage \n",
            "using data from the American Community Survey (ACS). The  \n",
            "U.S. Census Bureau conducts the ACS throughout the year; the \n",
            "survey asks respondents to report their coverage at the time of \n",
            "interview. The resulting measure of health insurance coverage, \n",
            "therefore, reflects an annual average of current comprehensive \n",
            "health insurance coverage status.* This uninsured rate measures a \n",
            "different concept than the measure based on the Current Population \n",
            "Survey Annual Social and Economic Supplement (CPS ASEC). \n",
            "For reporting purposes, the ACS broadly classifies health insurance \n",
            "coverage as private insurance or public insurance. The ACS defines \n",
            "private health insurance as a plan provided through an employer \n",
            "or a union, coverage purchased directly by an individual from an \n",
            "insurance company or through an exchange (such as healthcare.\n"
          ]
        }
      ],
      "source": [
        "## Query using Similarity Search\n",
        "query=\"WHAT IS HEALTH INSURANCE COVERAGE?\"\n",
        "relevant_docments=vectorstore.similarity_search(query)\n",
        "\n",
        "print(relevant_docments[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZNWB61Q1QeI",
        "outputId": "c1ac8ebd-314a-4b1b-8642-1066af425b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000159F4860C10> search_kwargs={'k': 3}\n"
          ]
        }
      ],
      "source": [
        "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
        "print(retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPGbrxSc1QeI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSK4kYNK1QeI"
      },
      "source": [
        "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n-N6zcC1QeJ",
        "outputId": "36d9c9f3-33af-4300-b802-cd4206d4c897"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'What is the health insurance coverage?\\n\\nThe health insurance coverage is a contract between the insurer and the insured. The insurer agrees to pay the insured for the medical expenses incurred by the insured. The insured agrees to pay the premium to the insurer.\\n\\nWhat is the health insurance coverage?\\n\\nThe health insurance coverage is a contract between the insurer and the insured. The insurer agrees to pay the insured for the medical expenses incurred by the insured.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "hf=HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
        "\n",
        ")\n",
        "query=\"What is the health insurance coverage?\"\n",
        "hf.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dn1CNxT1QeJ"
      },
      "outputs": [],
      "source": [
        "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
        ")\n",
        "\n",
        "llm = hf\n",
        "llm.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CofVDurJ1QeK"
      },
      "outputs": [],
      "source": [
        "prompt_template=\"\"\"\n",
        "Use the following piece of context to answer the question asked.\n",
        "Please try to provide the answer only based on the context\n",
        "\n",
        "{context}\n",
        "Question:{question}\n",
        "\n",
        "Helpful Answers:\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AfdQ3LW1QeK"
      },
      "outputs": [],
      "source": [
        "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaJqCMD11QeK"
      },
      "outputs": [],
      "source": [
        "retrievalQA=RetrievalQA.from_chain_type(\n",
        "    llm=hf,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\":prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A73YmL581QeK"
      },
      "outputs": [],
      "source": [
        "query=\"\"\"DIFFERENCES IN THE\n",
        "UNINSURED RATE BY STATE\n",
        "IN 2022\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq2KFZBR1QeK",
        "outputId": "ef8b943d-2abf-4217-a4fd-f9f99fc54ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Use the following piece of context to answer the question asked.\n",
            "Please try to provide the answer only based on the context\n",
            "\n",
            "comparison of ACS and CPS ASEC measures \n",
            "of health insurance coverage, refer to < www.\n",
            "census.gov/topics/health/health-insurance/\n",
            "guidance.html >.\n",
            "9 Respondents may have more than one \n",
            "health insurance coverage type at the time \n",
            "of interview. As a result, adding the total \n",
            "number of people with private coverage and \n",
            "the total number with public coverage will \n",
            "sum to more than the total number with any \n",
            "coverage.‚Ä¢ From 2021 to 2022, nine states \n",
            "reported increases in private \n",
            "coverage, while seven reported \n",
            "decreases (Appendix Table B-2). \n",
            "DIFFERENCES IN THE \n",
            "UNINSURED RATE BY STATE \n",
            "IN 2022\n",
            "In 2022, uninsured rates at the \n",
            "time of interview ranged across \n",
            "states from a low of 2.4 percent \n",
            "in Massachusetts to a high of 16.6 \n",
            "percent in Texas, compared to the \n",
            "national rate of 8.0 percent.10 Ten \n",
            "of the 15 states with uninsured \n",
            "10 The uninsured rates in the District \n",
            "of Columbia and Massachusetts were not \n",
            "statistically different.rates above the national aver -\n",
            "\n",
            "percent (Appendix Table B-5). \n",
            "Medicaid coverage accounted \n",
            "for a portion of that difference. \n",
            "Medicaid coverage was 22.7 per -\n",
            "cent in the group of states that \n",
            "expanded Medicaid eligibility and \n",
            "18.0 percent in the group of nonex -\n",
            "pansion states.\n",
            "CHANGES IN THE UNINSURED \n",
            "RATE BY STATE FROM 2021 \n",
            "TO 2022\n",
            "From 2021 to 2022, uninsured rates \n",
            "decreased across 27 states, while \n",
            "only Maine had an increase. The \n",
            "uninsured rate in Maine increased \n",
            "from 5.7 percent to 6.6 percent, \n",
            "although it remained below the \n",
            "national average. Maine‚Äôs uninsured \n",
            "rate was still below 8.0 percent, \n",
            "21 Douglas Conway and Breauna Branch, \n",
            "‚ÄúHealth Insurance Coverage Status and Type \n",
            "by Geography: 2019 and 2021,‚Äù 2022, < www.\n",
            "census.gov/content/dam/Census/library/\n",
            "publications/2022/acs/acsbr-013.pdf >.\n",
            "\n",
            "library/publications/2022/acs/acsbr-013.pdf >.\n",
            "39 In 2022, the private coverage rates were \n",
            "not statistically different in North Dakota and \n",
            "Utah.Figure /five.tab/period.tab\n",
            "Percentage of Uninsured People for the /two.tab/five.tab Most Populous Metropolitan \n",
            "Areas/colon.tab /two.tab/zero.tab/two.tab/one.tab and /two.tab/zero.tab/two.tab/two.tab\n",
            "(Civilian, noninstitutionalized population) /uni00A0\n",
            "* Denotes a statistically signiÔ¨Åcant change between 2021 and 2022 at the 90 percent conÔ¨Ådence level.\n",
            "Note: For information on conÔ¨Ådentiality protection, sampling error, nonsampling error, and deÔ¨Ånitions in the American Community\n",
            "Survey, refer to <https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/ACS_Accuracy_of_Data_2022.pdf>.\n",
            "Source: U.S. Census Bureau, 2021 and 2022 American Community Survey, 1-year estimates. Boston-Cambridge-Newton/comma.tab MA-NH\n",
            "San Francisco-Oakland-Berkeley/comma.tab CA\n",
            "*Detroit-Warren-Dearborn/comma.tab MI\n",
            "Question:DIFFERENCES IN THE\n",
            "UNINSURED RATE BY STATE\n",
            "IN 2022\n",
            "\n",
            "Helpful Answers:\n",
            " 1.\n",
            " 2.\n",
            " 3.\n",
            " 4.\n",
            " 5.\n",
            " 6.\n",
            " 7.\n",
            " 8.\n",
            " 9.\n",
            " 10.\n",
            " 11.\n",
            " 12.\n",
            " 13.\n",
            " 14.\n",
            " 15.\n",
            " 16.\n",
            " 17.\n",
            " 18.\n",
            " 19.\n",
            " 20.\n",
            " 21.\n",
            " 22.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the QA chain with our query.\n",
        "result = retrievalQA.invoke({\"query\": query})\n",
        "print(result['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc3IbUcj1QeK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIp_zWfc1QeK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
