{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidfdezmartin/Chatbot-con-FastAPI-Streamlit-y-LangChain/blob/main/Chatbot-con-FastAPI-Streamlit-y-LangChain/documentos_y_matcomplement/Copia_de_huggingface(a%20ver%20si%20deja%20editar).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí monto requeriments2.txt del GitHub pero com la libreria typer==0.09.0, que por un lado debe ser <0.10.00 (por eso he puesto 0.09.0 y por otro mayor que para fastapi-cli 0.0.4 requires typer>=0.12.3, a REVISARLO"
      ],
      "metadata": {
        "id": "avPoFH9T_X8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEgbWBvKTRBE",
        "outputId": "e99ba3c7-ef5f-408c-9823-bef87dd58cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si se ha instalado longchain y falla, quitar el # del comentario de esta celda"
      ],
      "metadata": {
        "id": "LeKeYrcC8o2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y langchain langchain-community\n",
        "!pip install -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkaYUOpwZ-d6",
        "outputId": "bcaae108-c953-4c14-9316-faffbd47ad8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.137 (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Using cached langchain-0.0.137-py3-none-any.whl (518 kB)\n",
            "Collecting python-dotenv (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 2))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting streamlit (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3))\n",
            "  Downloading streamlit-1.35.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 4))\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4 (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 5))\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 6)) (4.12.3)\n",
            "Collecting pypdf (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 7))\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 8))\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting groq (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9))\n",
            "  Downloading groq-0.8.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-groq (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 10))\n",
            "  Downloading langchain_groq-0.1.4-py3-none-any.whl (11 kB)\n",
            "Collecting fastapi (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 12))\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse_starlette (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 13))\n",
            "  Downloading sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
            "Collecting googletrans (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 14))\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiocache (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 15))\n",
            "  Downloading aiocache-0.12.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting aioredis (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 16))\n",
            "  Downloading aioredis-2.0.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (6.0.1)\n",
            "Collecting SQLAlchemy<2,>=1 (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Downloading SQLAlchemy-1.4.52-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (1.25.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (8.3.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (8.1.7)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (14.0.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (13.7.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (4.11.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3))\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3))\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_community (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 4))\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.52 (from langchain_community->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 4))\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 4))\n",
            "  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 6)) (2.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9)) (1.3.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11)) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting h11>=0.8 (from uvicorn->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 12))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of googletrans to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting googletrans (from -r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 14))\n",
            "  Downloading googletrans-2.4.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9)) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9)) (1.2.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9)) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 9))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11)) (2.1.5)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain_community->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 4))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<25,>=16.8 (from streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3))\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1)) (3.0.3)\n",
            "Collecting httptools>=0.5.0 (from uvicorn->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 12))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 12))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 12))\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 12))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain_community->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 4))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 3)) (1.16.0)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 11))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.137->-r /content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/requirements/requirements2.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.4.0-py3-none-any.whl size=15762 sha256=01572ab598f163b405713c2f05aa3409a70ab0bacc46f4ca1dbf2cb8d80c7c9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/5f/60/c4738a8b36085696062052befbbfb65fc94d2286fb17015856\n",
            "Successfully built googletrans\n",
            "Installing collected packages: aiocache, websockets, watchdog, uvloop, ujson, SQLAlchemy, smmap, shellingham, python-multipart, python-dotenv, pypdf, pydantic, packaging, orjson, mypy-extensions, jsonpointer, httptools, h11, faiss-cpu, dnspython, aioredis, watchfiles, uvicorn, typing-inspect, starlette, pydeck, openapi-schema-pydantic, marshmallow, langsmith, jsonpatch, httpcore, googletrans, gitdb, email_validator, bs4, typer, sse_starlette, langchain-core, httpx, gitpython, dataclasses-json, langchain_community, langchain, groq, fastapi-cli, streamlit, langchain-groq, fastapi\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.30\n",
            "    Uninstalling SQLAlchemy-2.0.30:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.30\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.1\n",
            "    Uninstalling pydantic-2.7.1:\n",
            "      Successfully uninstalled pydantic-2.7.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.52 which is incompatible.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-1.4.52 aiocache-0.12.2 aioredis-2.0.1 bs4-0.0.2 dataclasses-json-0.5.14 dnspython-2.6.1 email_validator-2.1.1 faiss-cpu-1.8.0 fastapi-0.111.0 fastapi-cli-0.0.4 gitdb-4.0.11 gitpython-3.1.43 googletrans-2.4.0 groq-0.8.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.137 langchain-core-0.1.52 langchain-groq-0.1.4 langchain_community-0.0.38 langsmith-0.1.63 marshmallow-3.21.2 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 orjson-3.10.3 packaging-23.2 pydantic-1.10.15 pydeck-0.9.1 pypdf-4.2.0 python-dotenv-1.0.1 python-multipart-0.0.9 shellingham-1.5.4 smmap-5.0.1 sse_starlette-2.1.0 starlette-0.37.2 streamlit-1.35.0 typer-0.12.3 typing-inspect-0.9.0 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 watchdog-4.0.1 watchfiles-0.22.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SI HACE FALTA WIKI-LANGCHAIN"
      ],
      "metadata": {
        "id": "R3ulmY2na_zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet wikipedia langchain langchain_community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDeVvW7iXd0z",
        "outputId": "960e4925-fec0-48ef-88f2-87365f1cf8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Wikipedia-API # Faltaba instalar este, añadir luego al requirements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGFfMIsA4NZU",
        "outputId": "006fdf95-01bb-4756-f755-09d93e4795e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wikipedia-API\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2024.2.2)\n",
            "Installing collected packages: Wikipedia-API\n",
            "Successfully installed Wikipedia-API-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AGENTE DE USUARIO PARA WIKIPEDIA"
      ],
      "metadata": {
        "id": "qNPGlxA3ar9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Configuración de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No page found for query: \" + query\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"Fibrosis\"\n",
        "result = get_wikipedia_summary(query)\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrTAmQg-avk8",
        "outputId": "909a83c9-f3e4-4654-ba58-3921070e0c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fibrosis, also known as fibrotic scarring, is a pathological wound healing in which connective tissue replaces normal parenchymal tissue to the extent that it goes unchecked, leading to considerable tissue remodelling and the formation of permanent scar tissue.\n",
            "Repeated injuries, chronic inflammation and repair are susceptible to fibrosis, where an accidental excessive accumulation of extracellular matrix components, such as the collagen, is produced by fibroblasts, leading to the formation of a permanent fibrotic scar.\n",
            "In response to injury, this is called scarring, and if fibrosis arises from a single cell line, this is called a fibroma. Physiologically, fibrosis acts to deposit connective tissue, which can interfere with or totally inhibit the normal architecture and function of the underlying organ or tissue. Fibrosis can be used to describe the pathological state of excess deposition of fibrous tissue, as well as the process of connective tissue deposition in healing. Defined by the pathological accumulation of extracellular matrix (ECM) proteins, fibrosis results in scarring and thickening of the affected tissue — it is in essence an exaggerated wound healing response which interferes with normal organ function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YPTCzk1ebdf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AHORA INTEGRAMOS LANGCHAIN CON WIKI Y PROBAMOS"
      ],
      "metadata": {
        "id": "-l9vWb27cmKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXS3LQ21deFF",
        "outputId": "d30d530e-ff60-4b9d-a802-c45f118a2389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m163.8/171.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence_transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRUEBA DE API KEY DE HUGGING FACE"
      ],
      "metadata": {
        "id": "QA-C3lf_esDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí o se sube e. .env de GitHub o se meten en el colab en la llave de la izquierda los API KEYS secretas, y se corre la siguiente celda"
      ],
      "metadata": {
        "id": "mzp6jh0JALtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "BsVoIrTmAH3x",
        "outputId": "0241ad7c-524d-4c63-c475-e1ca97581a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret secretName does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-245876ae012f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'secretName'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret secretName does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De momento me funciona mejor subiendo el .env de GitHub al root de aquí"
      ],
      "metadata": {
        "id": "ztVtxmQIAirG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv('/content/drive/MyDrive/Chatbot-con-FastAPI-Streamlit-y-LangChain/.env')\n",
        "add_to_git_credential=True\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    from huggingface_hub import login\n",
        "    login(hf_token)\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Obtener otras variables de entorno si es necesario\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "    print(\"GROQ_API_KEY cargado correctamente\")\n",
        "else:\n",
        "    print(\"GROQ_API_KEY no encontrado en el archivo .env\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdO1rIukertP",
        "outputId": "23549f46-defd-4ad5-ce0a-0121699d9ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "GROQ_API_KEY cargado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AHORA PRUEBA DE CODIGO BUENO\n",
        "Explicación\n",
        "VectorStoreRetriever: Se utiliza para configurar el recuperador basado en el vectorstore.\n",
        "load_qa_chain: Carga una cadena de preguntas y respuestas utilizando el PromptTemplate.\n",
        "RetrievalQA: Se configura con el retriever y combine_documents_chain necesarios.\n",
        "Pruebo con un pdf pasado a .md de David grabado a saco en el root (/content/sample_data). En PDF funcionaba bien, en MD.\n",
        "PRUEBA 1"
      ],
      "metadata": {
        "id": "u8pg-fC0e56F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CARGAR=\"/content/sample_data/Cholesterol-Myths-vs-Facts-Spanish.md\""
      ],
      "metadata": {
        "id": "JevHfNjqf3TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Configuración de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No page found for query: \" + query\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"FIBROSIS\"\n",
        "wikipedia_result = get_wikipedia_summary(query)\n",
        "\n",
        "print(\"Wikipedia Result:\\n\", wikipedia_result)\n",
        "\n",
        "# Configuración de LangChain y otros módulos\n",
        "# Cargar documentos\n",
        "CARGAR = \"/content/sample_data/Cholesterol-Myths-vs-Facts-Spanish.md\"\n",
        "# ojo md no pdf quito... pdf_loader = PyPDFLoader(CARGAR)\n",
        "# ojo si cargo un md ya no es un pdf, por eso cambia el codigo pej...documents =\n",
        "# pdf_loader.load()\n",
        "text_loader = TextLoader(CARGAR)\n",
        "documents = text_loader.load()\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta basándote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "qa_chain = load_qa_chain(prompt_template=prompt_template)\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas\n",
        "question = \"¿Cuál es el tema principal del documento?\"\n",
        "result = retrieval_qa.run(question)\n",
        "\n",
        "print(\"QA Result:\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "Asu3AEcsckst",
        "outputId": "814536dc-c006-4e7b-ded8-2ed448b7b0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia Result:\n",
            " No page found for query: FIBROSIS\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TextLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-596905dfb23d>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# ojo si cargo un md ya no es un pdf, por eso cambia el codigo pej...documents =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# pdf_loader.load()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtext_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCARGAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TextLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRUEBA 2. ASI CARGARIAMOS LOS PDF DE SU SUBDIRECTORIO"
      ],
      "metadata": {
        "id": "G-o4e4A-1Y28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "PXxZNfk41QeH",
        "outputId": "6b1fd0db-5df7-4edc-85e0-e7cf8cb3367f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PyPDFDirectoryLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a3bbfe67377>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Read the ppdfs from the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPyPDFDirectoryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PyPDFDirectoryLoader' is not defined"
          ]
        }
      ],
      "source": [
        "## Read the ppdfs from the folder\n",
        "loader=PyPDFDirectoryLoader(\"./pdf\")\n",
        "\n",
        "documents=loader.load()\n",
        "\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "\n",
        "final_documents=text_splitter.split_documents(documents)\n",
        "final_documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La anterior PRUEBA 1 (de PDF modificado)., lo rehago para cargar .md, y además sigo haciendo pruebas con WIKIPEDIA que fuciona perfecto."
      ],
      "metadata": {
        "id": "tkt3WOT4EGyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El error normal de longitud indica que el número de tokens en la entrada más los tokens generados exceden el límite de 1024 para el modelo gpt2. Para resolver esto, podemos:\n",
        "\n",
        "Reducir el tamaño del contexto enviado al modelo.\n",
        "Utilizar un modelo con un límite de tokens mayor."
      ],
      "metadata": {
        "id": "JUj79yYxHeeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader  # Asegúrate de que el import es correcto\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar las variables de entorno\n",
        "load_dotenv()\n",
        "\n",
        "# Obtener el token de Hugging Face desde las variables de entorno\n",
        "hf_token = os.getenv(\"HUGGING_FACE_API_TOKEN\")\n",
        "if hf_token:\n",
        "    print(\"Token de Hugging Face cargado correctamente.\")\n",
        "else:\n",
        "    print(\"HUGGING_FACE_API_TOKEN no encontrado en el archivo .env\")\n",
        "\n",
        "# Configuración de la API de Wikipedia con agente de usuario\n",
        "user_agent = 'MyApp/1.0 (example@example.com)'\n",
        "wiki_wiki = wikipediaapi.Wikipedia('en', headers={'User-Agent': user_agent})\n",
        "\n",
        "def get_wikipedia_summary(query):\n",
        "    page = wiki_wiki.page(query)\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return \"No page found for query: \" + query\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"Hunter x Hunter\"\n",
        "wikipedia_result = get_wikipedia_summary(query)\n",
        "\n",
        "print(\"Wikipedia Result:\\n\", wikipedia_result)\n",
        "\n",
        "# Configuración de LangChain y otros módulos\n",
        "# Cargar documentos\n",
        "CARGAR = \"/content/sample_data/Cholesterol-Myths-vs-Facts-Spanish.md\"\n",
        "\n",
        "# Leer el archivo de texto manualmente\n",
        "with open(CARGAR, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Crear una clase de documento simple\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "# Crear un objeto de documento\n",
        "documents = [Document(page_content=text)]\n",
        "\n",
        "# Dividir texto\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)  # Reducir el tamaño del fragmento\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Crear el prompt con variables de entrada especificadas\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"Responde la pregunta basándote en el siguiente contexto: {context}\\nPregunta: {question}\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Configurar el modelo de lenguaje de Hugging Face\n",
        "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=hf_token)\n",
        "\n",
        "# Crear la cadena de preguntas y respuestas\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Crear el retrieval QA chain utilizando el vectorstore y la cadena de QA\n",
        "retrieval_qa = RetrievalQA(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    combine_documents_chain=qa_chain\n",
        ")\n",
        "\n",
        "# Ejemplo de uso de la cadena de preguntas y respuestas\n",
        "question = \"¿Cuál es el tema principal del documento?\"\n",
        "result = retrieval_qa(question)\n",
        "\n",
        "print(\"QA Result:\\n\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljP-ISvlEF5-",
        "outputId": "dbd1090b-23a2-4c0e-ee93-f58088f0975c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token de Hugging Face cargado correctamente.\n",
            "Wikipedia Result:\n",
            " Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 37 tankōbon volumes as of November 2022. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\n",
            "Hunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\n",
            "The manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim's Toonami programming block from April 2016 to June 2019.\n",
            "Hunter × Hunter has been a huge critical and financial success and has become one of the best-selling manga series of all time, having over 84 million copies in circulation by July 2022.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QA Result:\n",
            " {'query': '¿Cuál es el tema principal del documento?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nde calorías. Además, tenga en cuenta que la porción en que se basan esos números puede ser menor\\n\\nque el contenido completo del envase. (Es decir, puede que el envase incluya más de una porción).\\n\\nMito: Cambiar la mantequilla por margarina me ayudará a bajar el colesterol.\\n\\nVerdad: No necesariamente. La mantequilla es alta en grasa saturada, pero algunos tipos de\\n\\nmargarinas tienen incluso un contenido mayor de ambos tipos de grasas. Las margarinas líquidas\\n\\nfactores de riesgos o un historial familiar de enfermedad cardíaca temprana. Después de los 20\\n\\naños, su doctor volverá a analizar su colesterol y otros factores de riesgo cada cuatro a seis años\\n\\nsiempre y cuando su riesgo se mantenga bajo.\\n\\nMito: Sólo la gente con sobrepeso u obesidad tiene colesterol alto.\\n\\nVerdad: Las personas con cualquier tipo de cuerpo pueden tener colesterol alto. Tener sobrepeso\\n\\nminutos a la semana, o una combinación de ambas, de preferencia esparcidas durante la semana.\\n\\nMito: Si la etiqueta de nutrición no muestra colesterol, el alimento es saludable para el corazón.\\n\\nVerdad: Muchos alimentos que “no tienen colesterol” o incluso que son “bajos en grasa” tienen un\\n\\nalto contenido de otros tipos de grasas “malas”, como las grasas saturadas y grasas trans. Asegúrese\\n\\nde revisar la etiqueta de los alimentos para ver si tienen grasa saturada y grasa trans, y ver el total\\n\\nevaluar su riesgo para toda la vida. Si tiene entre 40 y 75 años, pídale a su médico que evalúe su\\n\\nriesgo a 10 años. Si sus riesgos son altos, un cambio en su estilo de vida y medicamento de estatina\\n\\npodrían ayudar a controlar el riesgo.\\n\\nMito: Su nivel de colesterol es consecuencia de su dieta y su nivel de actividad física.\\n\\nVerdad: Es verdad, la dieta y la actividad física afectan su nivel de colesterol, pero no son los únicos\\n\\nQuestion: ¿Cuál es el tema principal del documento?\\nHelpful Answer: There are 13 documents that are part of the document. Each of these documents is a document that may be part of the document.\\n\\nDe cambio de la vida, el alimento de dos juego a los estos que équiparía, durante que la vida se proceso a conocido de una logaría\\n\\ncon el enfuerzo un vida.\\n\\nVerdad: ¿Cuál es\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VOY REVISANDO POR AQUÍ HASTA AHORA CARGA DE MD Y LLM DE HUGGING FACES PERFECTO Y WIKIPEDIA PERFECTA"
      ],
      "metadata": {
        "id": "a5m2xY9MHzB_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xQRZ6wW1QeI",
        "outputId": "e4f372df-66cd-4805-aa45-5a061bd86d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_documents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-af67d951323e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'final_documents' is not defined"
          ]
        }
      ],
      "source": [
        "len(final_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yypRm5wy1QeI",
        "outputId": "cde70545-8775-4c4f-9912-d4dc33264c6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "## Embedding Using Huggingface\n",
        "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\",      #sentence-transformers/all-MiniLM-l6-v2\n",
        "    model_kwargs={'device':'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings':True}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUpiY9gc1QeI",
        "outputId": "3c1b87e4-48ea-4826-f05f-ec34fe99923d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-8.46568644e-02 -1.19099217e-02 -3.37892585e-02  2.94559337e-02\n",
            "  5.19159958e-02  5.73839322e-02 -4.10017520e-02  2.74268016e-02\n",
            " -1.05128214e-01 -1.58056132e-02  7.94858634e-02  5.64318486e-02\n",
            " -1.31765157e-02 -3.41543928e-02  5.81604475e-03  4.72547710e-02\n",
            " -1.30746774e-02  3.12988879e-03 -3.44225690e-02  3.08406260e-02\n",
            " -4.09086421e-02  3.52737904e-02 -2.43761651e-02 -4.35831733e-02\n",
            "  2.41503324e-02  1.31986588e-02 -4.84452769e-03  1.92347374e-02\n",
            " -5.43912649e-02 -1.42735064e-01  5.15528210e-03  2.93115843e-02\n",
            " -5.60810715e-02 -8.53536371e-03  3.14141326e-02  2.76736189e-02\n",
            " -2.06188373e-02  8.24231505e-02  4.15425114e-02  5.79654835e-02\n",
            " -3.71587239e-02  6.26157876e-03 -2.41390076e-02 -5.61792636e-03\n",
            " -2.51715183e-02  5.04967198e-03 -2.52801236e-02 -2.91945064e-03\n",
            " -8.24046135e-03 -5.69604561e-02  2.30822619e-02 -5.54219959e-03\n",
            "  5.11555411e-02  6.09937944e-02  6.49766475e-02 -5.38514033e-02\n",
            "  2.19109710e-02 -2.54194364e-02 -4.49223518e-02  4.22459245e-02\n",
            "  4.75252084e-02  7.23217207e-04 -2.61084497e-01  9.30173397e-02\n",
            "  1.13597196e-02  4.90668938e-02 -1.06287086e-02 -8.08727555e-03\n",
            " -1.53562622e-02 -5.33785969e-02 -6.89967200e-02  4.75178175e-02\n",
            " -5.68596013e-02  9.38643236e-03  4.24065925e-02  2.54346598e-02\n",
            "  9.67096724e-03  7.90799968e-03  2.25160886e-02  1.91007671e-03\n",
            "  3.06091923e-02  2.43992303e-02 -1.34115480e-02 -4.77401279e-02\n",
            "  4.89939749e-02 -9.49416459e-02  5.62894158e-02 -4.76260781e-02\n",
            "  2.81447303e-02 -2.54329499e-02 -3.84951308e-02  1.00940112e-02\n",
            "  1.90582985e-04  3.36625241e-02  1.00181838e-02  2.83524077e-02\n",
            " -2.68963701e-03 -6.96359901e-03 -3.54914516e-02  3.42758894e-01\n",
            " -1.94496289e-02  1.43988123e-02 -5.68810571e-03  1.71480775e-02\n",
            " -2.88607483e-03 -5.81653193e-02  6.35178352e-04  5.17297909e-03\n",
            "  2.06331387e-02  1.65708251e-02  2.15096809e-02 -2.38796007e-02\n",
            "  2.89275143e-02  4.67319153e-02 -3.56104821e-02 -1.05078742e-02\n",
            "  3.70704755e-02  1.57502498e-02  9.43095461e-02 -2.50715371e-02\n",
            " -9.55965184e-03  1.78565886e-02 -9.41779092e-03 -4.57858741e-02\n",
            "  1.82930417e-02  5.81431836e-02  4.94311154e-02  1.46350682e-01\n",
            "  2.16057692e-02 -3.92896198e-02  1.03241250e-01 -3.48299928e-02\n",
            " -6.61871349e-03  7.07991235e-03  9.26990295e-04  4.49867966e-03\n",
            " -2.89777480e-02  4.02419232e-02 -5.23192994e-03  4.59961966e-02\n",
            "  4.23976406e-03 -4.83790739e-03 -3.23238224e-03 -1.41072914e-01\n",
            " -3.76811475e-02  1.83623895e-01 -2.96609867e-02  4.90660444e-02\n",
            "  3.90551575e-02 -1.57757346e-02 -3.86351012e-02  4.65630889e-02\n",
            " -2.43486036e-02  3.57695147e-02 -3.54947336e-02  2.36265883e-02\n",
            " -3.41984764e-04  3.11703496e-02 -2.39356644e-02 -5.94757982e-02\n",
            "  6.06259257e-02 -3.81902158e-02 -7.04255328e-02  1.42479865e-02\n",
            "  3.34432051e-02 -3.85255218e-02 -1.71951596e-02 -7.12288395e-02\n",
            "  2.64976211e-02  1.09495688e-02  1.32650323e-02  3.89527939e-02\n",
            "  1.60355382e-02 -3.17630395e-02  1.02013692e-01  2.92911958e-02\n",
            " -2.29205769e-02 -8.38053040e-03 -1.72173064e-02 -6.78820610e-02\n",
            "  5.39418403e-03 -2.32346989e-02 -6.07407168e-02 -3.86575758e-02\n",
            " -1.54306367e-02 -3.84982936e-02 -5.02867699e-02  5.04235253e-02\n",
            "  4.94898260e-02 -1.41083300e-02 -2.98144598e-03  9.76440133e-05\n",
            " -6.59190416e-02  3.01006734e-02 -5.46592870e-04 -1.64787639e-02\n",
            " -5.21614477e-02 -3.30222957e-03  4.75748219e-02 -3.40808518e-02\n",
            " -2.98659913e-02  2.75014956e-02  5.90203749e-03 -2.64040008e-03\n",
            " -1.61242671e-02  2.05222610e-02  1.21105220e-02 -5.49782291e-02\n",
            "  5.10389581e-02 -7.92088546e-03  7.25206453e-03  3.51751335e-02\n",
            "  3.66276912e-02  5.67682087e-04  2.60788649e-02  2.50971057e-02\n",
            "  1.14481300e-02 -2.54925024e-02  1.96417440e-02  2.84220409e-02\n",
            "  2.82554235e-02  6.57489598e-02  9.26554129e-02 -2.68629670e-01\n",
            " -8.90553114e-04  3.16917268e-03  5.08358562e-03 -6.42101169e-02\n",
            " -4.56614904e-02 -4.62259948e-02  3.60924639e-02  8.29056371e-03\n",
            "  8.92349407e-02  5.68022132e-02  6.91062305e-03 -1.08684311e-02\n",
            "  9.36060175e-02  1.03680165e-02 -8.60929266e-02  1.77331921e-02\n",
            " -2.00802702e-02 -1.85124874e-02  5.62404632e-04 -9.38337017e-03\n",
            "  7.76061229e-03 -5.37273772e-02 -2.30028406e-02  7.48890713e-02\n",
            " -1.29693234e-02  6.53716922e-02 -4.24983352e-02 -7.10293651e-02\n",
            " -1.56803615e-02 -6.23028576e-02  5.36034741e-02 -6.53212238e-03\n",
            " -1.15985490e-01  6.70967922e-02  1.93367060e-02 -6.67827874e-02\n",
            " -2.01754435e-03 -6.27636909e-02 -2.95005478e-02 -2.71986630e-02\n",
            "  4.49796803e-02 -6.61587194e-02  2.13751234e-02 -2.94077471e-02\n",
            " -5.71503267e-02  4.05282490e-02  7.11039603e-02 -6.80165365e-02\n",
            "  2.11908873e-02  1.30515285e-02 -2.91152820e-02 -2.25581583e-02\n",
            " -1.60188414e-02  3.20554040e-02 -5.89460693e-02 -2.97131632e-02\n",
            "  3.42681594e-02 -1.58376191e-02 -9.31771472e-03  3.59834097e-02\n",
            "  3.65337380e-03  4.73319739e-02 -1.06235184e-02 -8.69734772e-03\n",
            " -4.38009948e-02  5.94554143e-03 -2.41493657e-02 -7.79940188e-02\n",
            "  1.46542490e-02  1.05613861e-02  5.45365028e-02 -3.17896828e-02\n",
            " -1.26762642e-02  7.92561378e-03 -1.38133150e-02  5.01396768e-02\n",
            " -7.28576025e-03 -5.23702893e-03 -5.32640517e-02  4.78208959e-02\n",
            " -5.38353659e-02  1.11437477e-02  3.96674089e-02 -1.93496253e-02\n",
            "  9.94823873e-03 -3.53479455e-03  3.58561263e-03 -9.61500779e-03\n",
            "  2.15323791e-02 -1.82350334e-02 -2.15188656e-02 -1.38835954e-02\n",
            " -1.76698975e-02  3.38015234e-04 -3.84854589e-04 -2.25800529e-01\n",
            "  4.51243371e-02  1.53376386e-02 -1.76966917e-02 -1.42525993e-02\n",
            " -7.00285891e-03 -3.13724913e-02  2.13672244e-03 -9.28345323e-03\n",
            " -1.66986901e-02  4.66264114e-02  7.71809518e-02  1.26696959e-01\n",
            " -1.83595419e-02 -1.39637077e-02 -1.23306655e-03  5.93339056e-02\n",
            " -1.37463736e-03  1.98233537e-02 -2.92635858e-02  4.96656746e-02\n",
            " -6.07207343e-02  1.53544754e-01 -4.67309207e-02  1.97028890e-02\n",
            " -7.67833516e-02 -7.73231685e-03  3.71618718e-02 -3.00591048e-02\n",
            "  8.30263086e-03  2.06259061e-02  1.97466253e-03  3.39764208e-02\n",
            " -1.70869529e-02  4.84796055e-02  1.20781595e-02  1.24999117e-02\n",
            "  5.61724417e-02  9.88546945e-03  2.13879198e-02 -4.25293520e-02\n",
            " -1.94036588e-02  2.47837696e-02  1.37260715e-02  6.41119629e-02\n",
            " -2.84481011e-02 -4.64116633e-02 -5.36255538e-02 -6.95093986e-05\n",
            "  6.45710081e-02 -4.32005967e-04 -1.32471016e-02  5.85135119e-03\n",
            "  1.48595814e-02 -5.41847497e-02 -2.02038661e-02 -5.98262921e-02\n",
            "  3.67029347e-02  1.43319997e-03 -8.64463206e-03  2.90671345e-02\n",
            "  4.38365377e-02 -7.64942840e-02  1.55717917e-02  6.65831119e-02]\n",
            "(384,)\n"
          ]
        }
      ],
      "source": [
        "import  numpy as np\n",
        "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
        "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMWXH7-11QeI"
      },
      "outputs": [],
      "source": [
        "## VectorStore Creation\n",
        "vectorstore=FAISS.from_documents(final_documents[:120],huggingface_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SH9QYTK1QeI",
        "outputId": "233503b8-c137-45e5-95b3-83f47dc8a153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 U.S. Census Bureau\n",
            "WHAT IS HEALTH INSURANCE COVERAGE?\n",
            "This brief presents state-level estimates of health insurance coverage \n",
            "using data from the American Community Survey (ACS). The  \n",
            "U.S. Census Bureau conducts the ACS throughout the year; the \n",
            "survey asks respondents to report their coverage at the time of \n",
            "interview. The resulting measure of health insurance coverage, \n",
            "therefore, reflects an annual average of current comprehensive \n",
            "health insurance coverage status.* This uninsured rate measures a \n",
            "different concept than the measure based on the Current Population \n",
            "Survey Annual Social and Economic Supplement (CPS ASEC). \n",
            "For reporting purposes, the ACS broadly classifies health insurance \n",
            "coverage as private insurance or public insurance. The ACS defines \n",
            "private health insurance as a plan provided through an employer \n",
            "or a union, coverage purchased directly by an individual from an \n",
            "insurance company or through an exchange (such as healthcare.\n"
          ]
        }
      ],
      "source": [
        "## Query using Similarity Search\n",
        "query=\"WHAT IS HEALTH INSURANCE COVERAGE?\"\n",
        "relevant_docments=vectorstore.similarity_search(query)\n",
        "\n",
        "print(relevant_docments[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZNWB61Q1QeI",
        "outputId": "c1ac8ebd-314a-4b1b-8642-1066af425b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000159F4860C10> search_kwargs={'k': 3}\n"
          ]
        }
      ],
      "source": [
        "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
        "print(retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPGbrxSc1QeI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSK4kYNK1QeI"
      },
      "source": [
        "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n-N6zcC1QeJ",
        "outputId": "36d9c9f3-33af-4300-b802-cd4206d4c897"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'What is the health insurance coverage?\\n\\nThe health insurance coverage is a contract between the insurer and the insured. The insurer agrees to pay the insured for the medical expenses incurred by the insured. The insured agrees to pay the premium to the insurer.\\n\\nWhat is the health insurance coverage?\\n\\nThe health insurance coverage is a contract between the insurer and the insured. The insurer agrees to pay the insured for the medical expenses incurred by the insured.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "hf=HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
        "\n",
        ")\n",
        "query=\"What is the health insurance coverage?\"\n",
        "hf.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dn1CNxT1QeJ"
      },
      "outputs": [],
      "source": [
        "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
        ")\n",
        "\n",
        "llm = hf\n",
        "llm.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CofVDurJ1QeK"
      },
      "outputs": [],
      "source": [
        "prompt_template=\"\"\"\n",
        "Use the following piece of context to answer the question asked.\n",
        "Please try to provide the answer only based on the context\n",
        "\n",
        "{context}\n",
        "Question:{question}\n",
        "\n",
        "Helpful Answers:\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AfdQ3LW1QeK"
      },
      "outputs": [],
      "source": [
        "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaJqCMD11QeK"
      },
      "outputs": [],
      "source": [
        "retrievalQA=RetrievalQA.from_chain_type(\n",
        "    llm=hf,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\":prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A73YmL581QeK"
      },
      "outputs": [],
      "source": [
        "query=\"\"\"DIFFERENCES IN THE\n",
        "UNINSURED RATE BY STATE\n",
        "IN 2022\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq2KFZBR1QeK",
        "outputId": "ef8b943d-2abf-4217-a4fd-f9f99fc54ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Use the following piece of context to answer the question asked.\n",
            "Please try to provide the answer only based on the context\n",
            "\n",
            "comparison of ACS and CPS ASEC measures \n",
            "of health insurance coverage, refer to < www.\n",
            "census.gov/topics/health/health-insurance/\n",
            "guidance.html >.\n",
            "9 Respondents may have more than one \n",
            "health insurance coverage type at the time \n",
            "of interview. As a result, adding the total \n",
            "number of people with private coverage and \n",
            "the total number with public coverage will \n",
            "sum to more than the total number with any \n",
            "coverage.• From 2021 to 2022, nine states \n",
            "reported increases in private \n",
            "coverage, while seven reported \n",
            "decreases (Appendix Table B-2). \n",
            "DIFFERENCES IN THE \n",
            "UNINSURED RATE BY STATE \n",
            "IN 2022\n",
            "In 2022, uninsured rates at the \n",
            "time of interview ranged across \n",
            "states from a low of 2.4 percent \n",
            "in Massachusetts to a high of 16.6 \n",
            "percent in Texas, compared to the \n",
            "national rate of 8.0 percent.10 Ten \n",
            "of the 15 states with uninsured \n",
            "10 The uninsured rates in the District \n",
            "of Columbia and Massachusetts were not \n",
            "statistically different.rates above the national aver -\n",
            "\n",
            "percent (Appendix Table B-5). \n",
            "Medicaid coverage accounted \n",
            "for a portion of that difference. \n",
            "Medicaid coverage was 22.7 per -\n",
            "cent in the group of states that \n",
            "expanded Medicaid eligibility and \n",
            "18.0 percent in the group of nonex -\n",
            "pansion states.\n",
            "CHANGES IN THE UNINSURED \n",
            "RATE BY STATE FROM 2021 \n",
            "TO 2022\n",
            "From 2021 to 2022, uninsured rates \n",
            "decreased across 27 states, while \n",
            "only Maine had an increase. The \n",
            "uninsured rate in Maine increased \n",
            "from 5.7 percent to 6.6 percent, \n",
            "although it remained below the \n",
            "national average. Maine’s uninsured \n",
            "rate was still below 8.0 percent, \n",
            "21 Douglas Conway and Breauna Branch, \n",
            "“Health Insurance Coverage Status and Type \n",
            "by Geography: 2019 and 2021,” 2022, < www.\n",
            "census.gov/content/dam/Census/library/\n",
            "publications/2022/acs/acsbr-013.pdf >.\n",
            "\n",
            "library/publications/2022/acs/acsbr-013.pdf >.\n",
            "39 In 2022, the private coverage rates were \n",
            "not statistically different in North Dakota and \n",
            "Utah.Figure /five.tab/period.tab\n",
            "Percentage of Uninsured People for the /two.tab/five.tab Most Populous Metropolitan \n",
            "Areas/colon.tab /two.tab/zero.tab/two.tab/one.tab and /two.tab/zero.tab/two.tab/two.tab\n",
            "(Civilian, noninstitutionalized population) /uni00A0\n",
            "* Denotes a statistically signiﬁcant change between 2021 and 2022 at the 90 percent conﬁdence level.\n",
            "Note: For information on conﬁdentiality protection, sampling error, nonsampling error, and deﬁnitions in the American Community\n",
            "Survey, refer to <https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/ACS_Accuracy_of_Data_2022.pdf>.\n",
            "Source: U.S. Census Bureau, 2021 and 2022 American Community Survey, 1-year estimates. Boston-Cambridge-Newton/comma.tab MA-NH\n",
            "San Francisco-Oakland-Berkeley/comma.tab CA\n",
            "*Detroit-Warren-Dearborn/comma.tab MI\n",
            "Question:DIFFERENCES IN THE\n",
            "UNINSURED RATE BY STATE\n",
            "IN 2022\n",
            "\n",
            "Helpful Answers:\n",
            " 1.\n",
            " 2.\n",
            " 3.\n",
            " 4.\n",
            " 5.\n",
            " 6.\n",
            " 7.\n",
            " 8.\n",
            " 9.\n",
            " 10.\n",
            " 11.\n",
            " 12.\n",
            " 13.\n",
            " 14.\n",
            " 15.\n",
            " 16.\n",
            " 17.\n",
            " 18.\n",
            " 19.\n",
            " 20.\n",
            " 21.\n",
            " 22.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the QA chain with our query.\n",
        "result = retrievalQA.invoke({\"query\": query})\n",
        "print(result['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc3IbUcj1QeK"
      },
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIp_zWfc1QeK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}